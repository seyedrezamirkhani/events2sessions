{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Porting of example from http://czep.net/16/session-ids-sql.html to Spark SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"create or replace temporary view raw_events as ( \\\n",
    "    select * from ( \\\n",
    "        select 111 as user_id, timestamp '2016-05-01 17:00:00' as event_time union all \\\n",
    "        select 111 as user_id, timestamp '2016-05-01 17:01:00' as event_time union all \\\n",
    "        select 111 as user_id, timestamp '2016-05-01 17:02:00' as event_time union all \\\n",
    "        select 111 as user_id, timestamp '2016-05-01 17:03:00' as event_time union all \\\n",
    "        select 222 as user_id, timestamp '2016-05-01 18:00:00' as event_time union all \\\n",
    "        select 333 as user_id, timestamp '2016-05-01 19:00:00' as event_time union all \\\n",
    "        select 333 as user_id, timestamp '2016-05-01 19:10:00' as event_time union all \\\n",
    "        select 333 as user_id, timestamp '2016-05-01 19:20:00' as event_time union all \\\n",
    "        select 333 as user_id, timestamp '2016-05-01 19:30:00' as event_time union all \\\n",
    "        select 333 as user_id, timestamp '2016-05-01 20:01:00' as event_time union all \\\n",
    "        select 333 as user_id, timestamp '2016-05-01 20:02:00' as event_time union all \\\n",
    "        select 444 as user_id, timestamp '2016-05-01 23:01:00' as event_time union all \\\n",
    "        select 444 as user_id, timestamp '2016-05-01 23:21:00' as event_time union all \\\n",
    "        select 444 as user_id, timestamp '2016-05-01 23:59:00' as event_time union all \\\n",
    "        select 444 as user_id, timestamp '2016-05-02 00:01:00' as event_time union all \\\n",
    "        select 444 as user_id, timestamp '2016-05-02 00:21:00' as event_time union all \\\n",
    "        select 444 as user_id, timestamp '2016-05-02 23:59:00' as event_time union all \\\n",
    "        select 444 as user_id, timestamp '2016-05-03 00:05:00' as event_time \\\n",
    "    )\\\n",
    ")\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+\n",
      "|user_id|         event_time|\n",
      "+-------+-------------------+\n",
      "|    111|2016-05-01 17:00:00|\n",
      "|    111|2016-05-01 17:01:00|\n",
      "|    111|2016-05-01 17:02:00|\n",
      "|    111|2016-05-01 17:03:00|\n",
      "|    222|2016-05-01 18:00:00|\n",
      "|    333|2016-05-01 19:00:00|\n",
      "|    333|2016-05-01 19:10:00|\n",
      "|    333|2016-05-01 19:20:00|\n",
      "|    333|2016-05-01 19:30:00|\n",
      "|    333|2016-05-01 20:01:00|\n",
      "|    333|2016-05-01 20:02:00|\n",
      "|    444|2016-05-01 23:01:00|\n",
      "|    444|2016-05-01 23:21:00|\n",
      "|    444|2016-05-01 23:59:00|\n",
      "|    444|2016-05-02 00:01:00|\n",
      "|    444|2016-05-02 00:21:00|\n",
      "|    444|2016-05-02 23:59:00|\n",
      "|    444|2016-05-03 00:05:00|\n",
      "+-------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from raw_events\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"create or replace temporary view lagged_events as ( \\\n",
    "    select \\\n",
    "        user_id, \\\n",
    "        event_time, \\\n",
    "        lag(event_time) over (partition by date(event_time), user_id order by event_time) as prev \\\n",
    "    from \\\n",
    "        raw_events \\\n",
    ")\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+-------------------+\n",
      "|user_id|         event_time|               prev|\n",
      "+-------+-------------------+-------------------+\n",
      "|    111|2016-05-01 17:00:00|               null|\n",
      "|    111|2016-05-01 17:01:00|2016-05-01 17:00:00|\n",
      "|    111|2016-05-01 17:02:00|2016-05-01 17:01:00|\n",
      "|    111|2016-05-01 17:03:00|2016-05-01 17:02:00|\n",
      "|    444|2016-05-01 23:01:00|               null|\n",
      "|    444|2016-05-01 23:21:00|2016-05-01 23:01:00|\n",
      "|    444|2016-05-01 23:59:00|2016-05-01 23:21:00|\n",
      "|    444|2016-05-02 00:01:00|               null|\n",
      "|    444|2016-05-02 00:21:00|2016-05-02 00:01:00|\n",
      "|    444|2016-05-02 23:59:00|2016-05-02 00:21:00|\n",
      "|    222|2016-05-01 18:00:00|               null|\n",
      "|    444|2016-05-03 00:05:00|               null|\n",
      "|    333|2016-05-01 19:00:00|               null|\n",
      "|    333|2016-05-01 19:10:00|2016-05-01 19:00:00|\n",
      "|    333|2016-05-01 19:20:00|2016-05-01 19:10:00|\n",
      "|    333|2016-05-01 19:30:00|2016-05-01 19:20:00|\n",
      "|    333|2016-05-01 20:01:00|2016-05-01 19:30:00|\n",
      "|    333|2016-05-01 20:02:00|2016-05-01 20:01:00|\n",
      "+-------+-------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from lagged_events\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we use to_unixtimestamp to compare timestamps in seconds; in this 1800 represents 30 minutes which is used to group\n",
    "# log events into sessions\n",
    "spark.sql(\"create or replace temporary view new_sessions as ( \\\n",
    "    select \\\n",
    "        user_id, \\\n",
    "        event_time, \\\n",
    "        case \\\n",
    "            when prev is null then 1 \\\n",
    "            when to_unix_timestamp(event_time) - to_unix_timestamp(prev) > 1800 then 1 \\\n",
    "            else 0 \\\n",
    "        end as is_new_session \\\n",
    "    from \\\n",
    "        lagged_events \\\n",
    ")\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+--------------+\n",
      "|user_id|         event_time|is_new_session|\n",
      "+-------+-------------------+--------------+\n",
      "|    111|2016-05-01 17:00:00|             1|\n",
      "|    111|2016-05-01 17:01:00|             0|\n",
      "|    111|2016-05-01 17:02:00|             0|\n",
      "|    111|2016-05-01 17:03:00|             0|\n",
      "|    444|2016-05-01 23:01:00|             1|\n",
      "|    444|2016-05-01 23:21:00|             0|\n",
      "|    444|2016-05-01 23:59:00|             1|\n",
      "|    444|2016-05-02 00:01:00|             1|\n",
      "|    444|2016-05-02 00:21:00|             0|\n",
      "|    444|2016-05-02 23:59:00|             1|\n",
      "|    222|2016-05-01 18:00:00|             1|\n",
      "|    444|2016-05-03 00:05:00|             1|\n",
      "|    333|2016-05-01 19:00:00|             1|\n",
      "|    333|2016-05-01 19:10:00|             0|\n",
      "|    333|2016-05-01 19:20:00|             0|\n",
      "|    333|2016-05-01 19:30:00|             0|\n",
      "|    333|2016-05-01 20:01:00|             1|\n",
      "|    333|2016-05-01 20:02:00|             0|\n",
      "+-------+-------------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from new_sessions\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"create or replace temporary view session_index as ( \\\n",
    "    select \\\n",
    "        user_id, \\\n",
    "        event_time, \\\n",
    "        is_new_session, \\\n",
    "        sum(is_new_session) over (partition by user_id order by event_time rows between unbounded preceding and current row) as session_index \\\n",
    "    from \\\n",
    "        new_sessions \\\n",
    ")\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+--------------+-------------+\n",
      "|user_id|         event_time|is_new_session|session_index|\n",
      "+-------+-------------------+--------------+-------------+\n",
      "|    333|2016-05-01 19:00:00|             1|            1|\n",
      "|    333|2016-05-01 19:10:00|             0|            1|\n",
      "|    333|2016-05-01 19:20:00|             0|            1|\n",
      "|    333|2016-05-01 19:30:00|             0|            1|\n",
      "|    333|2016-05-01 20:01:00|             1|            2|\n",
      "|    333|2016-05-01 20:02:00|             0|            2|\n",
      "|    222|2016-05-01 18:00:00|             1|            1|\n",
      "|    111|2016-05-01 17:00:00|             1|            1|\n",
      "|    111|2016-05-01 17:01:00|             0|            1|\n",
      "|    111|2016-05-01 17:02:00|             0|            1|\n",
      "|    111|2016-05-01 17:03:00|             0|            1|\n",
      "|    444|2016-05-01 23:01:00|             1|            1|\n",
      "|    444|2016-05-01 23:21:00|             0|            1|\n",
      "|    444|2016-05-01 23:59:00|             1|            2|\n",
      "|    444|2016-05-02 00:01:00|             1|            3|\n",
      "|    444|2016-05-02 00:21:00|             0|            3|\n",
      "|    444|2016-05-02 23:59:00|             1|            4|\n",
      "|    444|2016-05-03 00:05:00|             1|            5|\n",
      "+-------+-------------------+--------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from session_index\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+-------+-------------------+--------------+-------------+\n",
      "|user_id_session_index|user_id|         event_time|is_new_session|session_index|\n",
      "+---------------------+-------+-------------------+--------------+-------------+\n",
      "|                111.1|    111|2016-05-01 17:00:00|             1|            1|\n",
      "|                111.1|    111|2016-05-01 17:01:00|             0|            1|\n",
      "|                111.1|    111|2016-05-01 17:02:00|             0|            1|\n",
      "|                111.1|    111|2016-05-01 17:03:00|             0|            1|\n",
      "|                222.1|    222|2016-05-01 18:00:00|             1|            1|\n",
      "|                333.1|    333|2016-05-01 19:00:00|             1|            1|\n",
      "|                333.1|    333|2016-05-01 19:10:00|             0|            1|\n",
      "|                333.1|    333|2016-05-01 19:20:00|             0|            1|\n",
      "|                333.1|    333|2016-05-01 19:30:00|             0|            1|\n",
      "|                333.2|    333|2016-05-01 20:01:00|             1|            2|\n",
      "|                333.2|    333|2016-05-01 20:02:00|             0|            2|\n",
      "|                444.1|    444|2016-05-01 23:01:00|             1|            1|\n",
      "|                444.1|    444|2016-05-01 23:21:00|             0|            1|\n",
      "|                444.2|    444|2016-05-01 23:59:00|             1|            2|\n",
      "|                444.3|    444|2016-05-02 00:01:00|             1|            3|\n",
      "|                444.3|    444|2016-05-02 00:21:00|             0|            3|\n",
      "|                444.4|    444|2016-05-02 23:59:00|             1|            4|\n",
      "|                444.5|    444|2016-05-03 00:05:00|             1|            5|\n",
      "+---------------------+-------+-------------------+--------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# changed:\n",
    "# concat in place of || operator\n",
    "# string in place of varchar\n",
    "spark.sql(\"\\\n",
    "    select \\\n",
    "        concat(concat(cast(user_id as string), '.'), cast(session_index as string)) as user_id_session_index, \\\n",
    "        user_id, \\\n",
    "        event_time, \\\n",
    "        is_new_session, \\\n",
    "        session_index \\\n",
    "    from \\\n",
    "        session_index \\\n",
    "    order by  \\\n",
    "        user_id, event_time \\\n",
    "\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
